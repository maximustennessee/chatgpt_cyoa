{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This sample choose you own adventure (CYOA) application demonstrates using chains to accomplish the following:\n",
    "1) Use the LangChain python library to implement chained queries with OpenAI Chatbot \n",
    "2) Use private context taken from the web - loading inventory from dndbeyond.com \n",
    "3) Implement Retrieval Augmented Generation (RAG) using in-memory vectorstore called FAISS\n",
    "4) Use chains to implement history aware queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the retrieval chain and database\n",
    "1) Scrape inventory from dndbeyond.com using WebBaseLoader and beautifulsoup4\n",
    "2) Load invoentory into faiss vector store database\n",
    "3) Use OpenAI embedding to measure relatedness of strings in vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader('https://www.dndbeyond.com/equipment?filter-search=Weapon')\n",
    "inventory = loader.load()\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "documents = text_splitter.split_documents(inventory)\n",
    "\n",
    "vectorstore = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load armory into embeddings\n",
    "1) Create chain to pass Document to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "template = \"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test that the document chain works\n",
    "1) Pass a Document to document_chain\n",
    "2) Model executes prompt using Document as context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "document_chain.invoke({\n",
    "    \"input\":\"List the names of the first 3 weapons\",\n",
    "    \"context\": [Document(page_content=\"List of weapons: 1) Axe 2) Slingshot, 3) Halbred 4) Sword 5) Javelin\")]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement retrieval chain\n",
    "1) Use vectorstore as Document source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "vectorstore_retriever = vectorstore.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(vectorstore_retriever, document_chain)\n",
    "\n",
    "response = retrieval_chain.invoke({\n",
    "    \"input\" : \"List the names of all the available weapons in the following the format: 1. Weapon 1\\n2. Weapon 2.\"\n",
    "})\n",
    "\n",
    "# We'll use this later\n",
    "ai_message = response.get('answer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test retrieval chain works\n",
    "1) Use document store as source for model to execute input query\n",
    "2) 'context' contains relevant Documents from vectorstore\n",
    "3) 'answer' should contain result from llm to input question\n",
    "4) Parse the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response\n",
    "\n",
    "# Uncomment to see parsed inventory\n",
    "# response.get('answer').split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversation aware vectorstore retriever chain \n",
    "1) Maintain context for searching relevant embedding in vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "\n",
    "retriever_chain = create_history_aware_retriever(llm, vectorstore_retriever, prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversation aware document retriever chain\n",
    "1) Maintain for document retriever query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Answer the users questions based on the below contenxt:\\n\\n{context}\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final product - here's the beauty of it\n",
    "1) Create our final conversation aware vectorstore retriever chain\n",
    "2) Chat history will be used in queries for both embedding AND the returned documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_retrieval_chain = create_retrieval_chain(retriever_chain, document_chain)\n",
    "\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "chat_history = [\n",
    "    HumanMessage(content=\"List the names of all the available weapons in the following the format: 1. Weapon 1\\n2. Weapon 2.\"),\n",
    "    AIMessage(content=ai_message)\n",
    "]\n",
    "response = conversation_retrieval_chain.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"List the names of all the ranged weapons, and their weights, in following the format: 1. Weapon 1, (WEIGHT)\\n2. Weapon 2 (WEIGHT)\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.get('answer'))\n",
    "\n",
    "inventory_raw = response.get('answer').split('\\n')\n",
    "\n",
    "# [item for item in inventory_raw if any (w in item for w in ['Melee Weapon', 'Ranged Weapon'])]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
